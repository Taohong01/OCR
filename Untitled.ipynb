{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[-1 -1]\n",
      " [-2 -1]\n",
      " [-3 -2]\n",
      " [ 1  1]\n",
      " [ 2  1]\n",
      " [ 3  2]]\n",
      "Help on PCA in module sklearn.decomposition.pca object:\n",
      "\n",
      "class PCA(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  Principal component analysis (PCA)\n",
      " |  \n",
      " |  Linear dimensionality reduction using Singular Value Decomposition of the\n",
      " |  data and keeping only the most significant singular vectors to project the\n",
      " |  data to a lower dimensional space.\n",
      " |  \n",
      " |  This implementation uses the scipy.linalg implementation of the singular\n",
      " |  value decomposition. It only works for dense arrays and is not scalable to\n",
      " |  large dimensional data.\n",
      " |  \n",
      " |  The time complexity of this implementation is ``O(n ** 3)`` assuming\n",
      " |  n ~ n_samples ~ n_features.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int, None or string\n",
      " |      Number of components to keep.\n",
      " |      if n_components is not set all components are kept::\n",
      " |  \n",
      " |          n_components == min(n_samples, n_features)\n",
      " |  \n",
      " |      if n_components == 'mle', Minka's MLE is used to guess the dimension\n",
      " |      if ``0 < n_components < 1``, select the number of components such that\n",
      " |      the amount of variance that needs to be explained is greater than the\n",
      " |      percentage specified by n_components\n",
      " |  \n",
      " |  copy : bool\n",
      " |      If False, data passed to fit are overwritten and running\n",
      " |      fit(X).transform(X) will not yield the expected results,\n",
      " |      use fit_transform(X) instead.\n",
      " |  \n",
      " |  whiten : bool, optional\n",
      " |      When True (False by default) the `components_` vectors are divided\n",
      " |      by n_samples times singular values to ensure uncorrelated outputs\n",
      " |      with unit component-wise variances.\n",
      " |  \n",
      " |      Whitening will remove some information from the transformed signal\n",
      " |      (the relative variance scales of the components) but can sometime\n",
      " |      improve the predictive accuracy of the downstream estimators by\n",
      " |      making there data respect some hard-wired assumptions.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  `components_` : array, [n_components, n_features]\n",
      " |      Components with maximum variance.\n",
      " |  \n",
      " |  `explained_variance_ratio_` : array, [n_components]\n",
      " |      Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0\n",
      " |  \n",
      " |  `mean_` : array, [n_features]\n",
      " |      Per-feature empirical mean, estimated from the training set.\n",
      " |  \n",
      " |  `n_components_` : int\n",
      " |      The estimated number of components. Relevant when n_components is set\n",
      " |      to 'mle' or a number between 0 and 1 to select using explained\n",
      " |      variance.\n",
      " |  \n",
      " |  `noise_variance_` : float\n",
      " |      The estimated noise covariance following the Probabilistic PCA model\n",
      " |      from Tipping and Bishop 1999. See \"Pattern Recognition and\n",
      " |      Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n",
      " |      http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n",
      " |      computed the estimated data covariance and score samples.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For n_components='mle', this class uses the method of `Thomas P. Minka:\n",
      " |  Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`\n",
      " |  \n",
      " |  Implements the probabilistic PCA model from:\n",
      " |  M. Tipping and C. Bishop, Probabilistic Principal Component Analysis,\n",
      " |  Journal of the Royal Statistical Society, Series B, 61, Part 3, pp. 611-622\n",
      " |  via the score and score_samples methods.\n",
      " |  See http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |  \n",
      " |  Due to implementation subtleties of the Singular Value Decomposition (SVD),\n",
      " |  which is used in this implementation, running fit twice on the same matrix\n",
      " |  can lead to principal components with signs flipped (change in direction).\n",
      " |  For this reason, it is important to always use the same estimator object to\n",
      " |  transform data in a consistent fashion.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.decomposition import PCA\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> pca = PCA(n_components=2)\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(copy=True, n_components=2, whiten=False)\n",
      " |  >>> print(pca.explained_variance_ratio_) # doctest: +ELLIPSIS\n",
      " |  [ 0.99244...  0.00755...]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  ProbabilisticPCA\n",
      " |  RandomizedPCA\n",
      " |  KernelPCA\n",
      " |  SparsePCA\n",
      " |  TruncatedSVD\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PCA\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=None, copy=True, whiten=False)\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the model with X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples in the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit the model with X and apply the dimensionality reduction on X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array-like, shape (n_samples, n_components)\n",
      " |  \n",
      " |  get_covariance(self)\n",
      " |      Compute data covariance with the generative model.\n",
      " |      \n",
      " |      ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\n",
      " |      where  S**2 contains the explained variances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cov : array, shape=(n_features, n_features)\n",
      " |          Estimated covariance of data.\n",
      " |  \n",
      " |  get_precision(self)\n",
      " |      Compute data precision matrix with the generative model.\n",
      " |      \n",
      " |      Equals the inverse of the covariance but computed with\n",
      " |      the matrix inversion lemma for efficiency.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      precision : array, shape=(n_features, n_features)\n",
      " |          Estimated precision of data.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Transform data back to its original space, i.e.,\n",
      " |      return an input X_original whose transform would be X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_components)\n",
      " |          New data, where n_samples is the number of samples\n",
      " |          and n_components is the number of components.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_original array-like, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If whitening is enabled, inverse_transform does not compute the\n",
      " |      exact inverse operation as transform.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Return the average log-likelihood of all samples\n",
      " |      \n",
      " |      See. \"Pattern Recognition and Machine Learning\"\n",
      " |      by C. Bishop, 12.2.1 p. 574\n",
      " |      or http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: array, shape(n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll: float\n",
      " |          Average log-likelihood of the samples under the current model\n",
      " |  \n",
      " |  score_samples(self, X)\n",
      " |      Return the log-likelihood of each sample\n",
      " |      \n",
      " |      See. \"Pattern Recognition and Machine Learning\"\n",
      " |      by C. Bishop, 12.2.1 p. 574\n",
      " |      or http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: array, shape(n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll: array, shape (n_samples,)\n",
      " |          Log-likelihood of each sample under the current model\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Apply the dimensionality reduction on X.\n",
      " |      \n",
      " |      X is projected on the first principal components previous extracted\n",
      " |      from a training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          New data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array-like, shape (n_samples, n_components)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep: boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The former have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "\n",
      "\n",
      "variance ratio\n",
      "\n",
      "[ 0.99244289  0.00755711]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "print 'X:\\n', X\n",
    "pca = PCA(n_components=2)\n",
    "help(pca)\n",
    "\n",
    "pca.fit(X)\n",
    "print\n",
    "print\n",
    "print 'variance ratio\\n'\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noah's program\n",
    "Noah says he doesn't know what is going to be yet.\n",
    "Yeah, yeah. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "x =200+200\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599\n"
     ]
    }
   ],
   "source": [
    "print 44 +555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print 2000 + 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999999\n"
     ]
    }
   ],
   "source": [
    "print 22222222 + 77777777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379436392523350380202020202019373606042000\n"
     ]
    }
   ],
   "source": [
    "print 15000 + 1123465799000*337737377373737373737373737373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "print 12*4+54-45\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
